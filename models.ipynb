{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I buy a lot of clothes online, and I've noticed websites making an effort to show more diverse models. In recent months I've seen models of all sizes and races, when even a few years ago, the models all looked pretty much the same. However, this change isn't happening on all websites. Just from browsing, it's pretty clear that some websites are a lot more inclusive than others.\n",
    "\n",
    "But... that's just anecdotal evidence. I'm more interested in a data-based answer. I love graphs-- how can I make a graph out of the skin tones of the models on a website? ([Click here to skip the process and see the graphs.](#graphs))\n",
    "\n",
    "For a more manageable question I'll focus on the women's section of two websites that have different audiences: American Eagle, which sells affordable, casual clothes for teens and young adults, and Gucci, which is a high fashion company. The former has been actively touting their diversity with hashtags and email campaigns; the latter is not making nearly the same effort. Let's take a statistical look at how racially diverse these websites are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/get_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to getting models' skin tones is to get pictures of them. This is a simple as grabbing every image from the women's section of the website.\n",
    "\n",
    "Web-scraping is a finnicky process that has to deal with a lot of site-specific JavaScript and CSS. I can write a generalizable function to get the image tags from each website, and another one to save the images to my machine, but to actually sift through the site's code, each website will have to be handled separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\":'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(site, headers=HEADERS):\n",
    "    page = requests.get(site, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    img_tags = soup.find_all(\"img\")\n",
    "    return img_tags\n",
    "\n",
    "def scrape_site(page_list, headers=HEADERS):\n",
    "    all_tags = []\n",
    "    for page in page_list:\n",
    "        tags = scrape_page(page, headers)\n",
    "        all_tags += tags\n",
    "    return all_tags\n",
    "\n",
    "def save_image(file_name, image_url):\n",
    "    try:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            full_url = \"http:{}?\".format(image_url)\n",
    "            response = requests.get(full_url)\n",
    "            pic = response.content\n",
    "            f.write(pic)\n",
    "    except:\n",
    "        print(\"Oops! The following URL failed to save:\\n{}\".format(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For American Eagle's website, I've only taken the pages that have images with the model's faces in them. For example, the \"tailgate\" section features mainly clothes hanging on a rack, and the \"jeans\" section has images that are taken from the waist down, so these pages have both been excluded when scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_PAGES = [\n",
    "    \"https://www.ae.com/women-tops/web/s-cat/10049?cm=sUS-cUSD&navdetail=mega:womens:c1:p2\"\n",
    "    \"https://www.ae.com/women-dresses/web/s-cat/1320034?cm=sUS-cUSD&navdetail=mega:womens:c1:p5\",\n",
    "    \"https://www.ae.com/jumpsuits-rompers-women/web/s-cat/8490001?cm=sUS-cUSD&navdetail=mega:womens:c1:p6\",\n",
    "    \"https://www.ae.com/matching-sets-women/web/s-cat/8420021?cm=sUS-cUSD&navdetail=mega:womens:c1:p7\",\n",
    "    \"https://www.ae.com/women-don-t-ask-why/web/s-cat/6570005?cm=sUS-cUSD&navdetail=mega:womens:c1:p9\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded 5119 images from American Eagle's website\n"
     ]
    }
   ],
   "source": [
    "ae_tags = scrape_site(AE_PAGES)\n",
    "print(\"Recorded {} images from American Eagle's website\".format(len(ae_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3273 American Eagle images\n"
     ]
    }
   ],
   "source": [
    "def process_ae_image(tag, count):\n",
    "    file_name = \"all_pics/scraped/ae/ae{0:04d}.jpg\".format(count)\n",
    "    try:\n",
    "        full_url = tag[\"data-srcset\"]\n",
    "        url = full_url.split(\"?\")[0]\n",
    "        save_image(file_name, url)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "count = 0\n",
    "for tag in ae_tags:\n",
    "    image_saved = process_ae_image(tag, count)\n",
    "    if (image_saved):\n",
    "        count += 1\n",
    "        \n",
    "print(\"Saved {} American Eagle images\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of pictures that were recorded but not saved! These pictures are basically html garbage that is irrelevant to this project. A lot of them are logos, or swatches of color, and the `process_ae_image` function makes sure that they aren't saved. We're left with over 3,000 pictures of clothing and models from the American Eagle site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gucci website is processed in a similar way, so that header and logo images aren't saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUCCI_PAGES = [\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-dresses-c-women-readytowear-dresses/1\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-coats-c-women-readytowear-coats\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-leather-casual-jackets-c-women-readytowear-leather-and-casual-jackets\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-jackets-c-women-readytowear-jackets\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-tops-shirts-c-women-readytowear-tops-and-shirts/1\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-sweaters-cardigans-c-women-readytowear-sweaters-and-cardigans\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-sweatshirts-t-shirts-c-women-readytowear-sweatshirts-and-tshirts/2\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-activewear-c-women-readytowear-activewear\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-skirts-c-women-readytowear-skirts/1\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-pants-shorts-c-women-readytowear-pants-and-shorts/1\",\n",
    "    \"https://www.gucci.com/us/en/ca/women/womens-ready-to-wear/womens-denim-c-women-readytowear-denim\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded 1389 images from Gucci's website\n"
     ]
    }
   ],
   "source": [
    "gucci_tags = scrape_site(GUCCI_PAGES)\n",
    "print(\"Recorded {} images from Gucci's website\".format(len(gucci_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1092 Gucci images\n"
     ]
    }
   ],
   "source": [
    "def process_gucci_image(tag, count):\n",
    "    file_name = \"all_pics/scraped/gucci/gucci{0:04d}.jpg\".format(count)\n",
    "    try:\n",
    "        url = tag[\"src\"]\n",
    "    except:\n",
    "        try:\n",
    "            url = tag[\"data-lazy-load-src\"]\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    if (\"snapchat\" in url):\n",
    "        return False\n",
    "    else:\n",
    "        save_image(file_name, url)\n",
    "        return True\n",
    "    \n",
    "count = 0\n",
    "for tag in gucci_tags:\n",
    "    image_saved = process_gucci_image(tag, count)\n",
    "    if (image_saved):\n",
    "        count += 1\n",
    "        \n",
    "print(\"Saved {} Gucci images\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/get_faces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I considered a lot of ways to extract the skin from these images. At first, I wanted to use a technique like [this](https://www.pyimagesearch.com/2014/08/18/skin-detection-step-step-example-using-python-opencv/) to get skin from any image-- even if it's just some ankles in a picture of jeans.\n",
    "\n",
    "However, skin detection is tricky: it requires you to make assumptions about the person you are analyzing in order to determine upper and lower boundaries for colors that are similar to their skin tone. [Darker skin is not recorded well in pictures](https://www.youtube.com/watch?v=d16LNHIEJzs), and as a result, detecting skin for many people with a wide range of skin tones is tricky.\n",
    "\n",
    "There's a workaround: rather than searching for skin, I can search for faces. [Facial recognition technology isn't perfect either](https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28), but its inconsistencies lie more in matching an image with an existing person, and less in pinpointing a face in an image. In other words, it'll be a better fit for this project. [`OpenCV`](https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html) has some great classifiers that I've made use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image, cascade_path=\"haarcascade_frontalface_default.xml\"):\n",
    "    face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "            grayscale_image,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5)\n",
    "    return faces\n",
    "\n",
    "def get_faces_in_image(image_path, company, image_number):\n",
    "    image = cv2.imread(image_path)\n",
    "    print(image_path)\n",
    "    faces = detect_faces(image)\n",
    "    face_number = 0\n",
    "    for (x, y, width, height) in faces:\n",
    "        cropped = image[y:y + height, x:x + width]\n",
    "        file_name = \"all_pics/cropped_faces/{0}/{1}{2:04d}_{3}.jpg\".format(company, company, image_number, face_number)\n",
    "        cv2.imwrite(file_name, cropped)\n",
    "        face_number += 1\n",
    "    \n",
    "    # were there any faces found?\n",
    "    return (face_number != 0)\n",
    "    \n",
    "def get_all_faces(company):\n",
    "    directory = \"all_pics/scraped/{}\".format(company)\n",
    "    image_number = 0\n",
    "    for image in os.listdir(directory):\n",
    "        image_path = \"{}/{}\".format(directory, image)\n",
    "        found_faces = get_faces_in_image(image_path, company, image_number)\n",
    "        if (found_faces):\n",
    "            image_number += 1\n",
    "    return image_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get the faces from the American Eagle models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1181 faces in American Eagle photos\n"
     ]
    }
   ],
   "source": [
    "faces = get_all_faces(\"ae\")\n",
    "print(\"Found {} faces in American Eagle photos\".format(faces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the Gucci faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 369 faces in Gucci photos\n"
     ]
    }
   ],
   "source": [
    "faces = get_all_faces(\"gucci\")\n",
    "print(\"Found {} faces in Gucci photos\".format(faces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting faces is difficult, and the classification isn't always perfect. Scrolling through the folder of saved faces, it's easy to see that there are a few errors. I've flagged a lot of them by marking the number of faces found per picture. If two faces were detected in a single photo, there's been a mistake. So any image named `(ae|gucci)XXXX_1.jpg` should be checked manually. Below are a few of the images that were classified as faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/not_faces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you're wondering: if I'm checking the photos manually for errors, what's the point of using automated facial classification anyway?\n",
    "\n",
    "The time-consuming part of grabbing all of the faces from all of these models actually isn't facial recognition. Detecting faces is the kind of complicated task that humans are really good at: if someone flashed an image at me, I could tell you if it has a face a lot more quickly than most computer programs can. For me, the time consuming part would be opening an image, cropping it down to the right size, and saving the new image. That might take me as long as a few minutes per picture. However, cropping and saving images is the kind of tedious task that can be easily automated with a Python script. The task can be finished in a few minutes rather than several hours by coding almost everything, and double-checking the output.\n",
    "\n",
    "After the Mistake Faces have all been deleted, I can run a quick script that will rename all the images so that they're in numerical order again. Another task where Python performs a lot more efficiently than humans!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(company):\n",
    "    directory = \"all_pics/cropped_faces/{}\".format(company)\n",
    "    count = 0\n",
    "    for image in os.listdir(directory):\n",
    "        file_name = \"all_pics/cropped_faces_renamed/{0}/{1}{2:04d}.jpg\".format(company, company, count)\n",
    "        image_path = \"{}/{}\".format(directory, image)\n",
    "        image = cv2.imread(image_path)\n",
    "        cv2.imwrite(file_name, image)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename(\"ae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename(\"gucci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each image has been reduced to a face, it's time to identify the skin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/get_skin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is just a list of pixels (technically, a 2d array of pixels), and each pixel is a tiny square of solid color. There are a few ways to numerically represent color, but the one we'll use here is BGR: each pixel can be reduced to three integers that are the amount of blue, red, and green mixed to form that color. The integers range from 0 to 255. (Sometimes this is referred to as RGB.)\n",
    "\n",
    "Since an image is just a collection of numbers, we can manipulate images with numerical operations. Consider taking the mean a photo that is predominantly skin (like a face!). Averaging together all of its pixels will give back one specific BGR value, which can approximate the skin tone of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_average(image):\n",
    "    average_row_color = np.average(image, axis=0)\n",
    "    average_color = np.average(average_row_color, axis=0)\n",
    "    color_block = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "    color_block[:, :] = average_color\n",
    "    return color_block  \n",
    "\n",
    "def get_average_colors(company):\n",
    "    directory = \"all_pics/cropped_faces_renamed/{}\".format(company)\n",
    "    for image in os.listdir(directory):\n",
    "        image_path = \"{}/{}\".format(directory, image)\n",
    "        face = cv2.imread(image_path)\n",
    "        average_color = take_average(face)\n",
    "        file_name = \"all_pics/average_color/{}/{}\".format(company, image)\n",
    "        cv2.imwrite(file_name, average_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_colors(\"ae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_colors(\"gucci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/average_colors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this looks really cool! But... something seems off. A lot of the boxes are *really* dark. And others have a grayish tone that doesn't seem like a skin color. Let's take a look at some of the images up close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/average_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average colors actually don't approximate the skin tones of the models at all. There are too many other elements getting in the way. In the images above, the models' hair and accessories pull the average away from the skin tone. To more concisely state the problem: there are too many non-skin pixels. \n",
    "\n",
    "Averaging would work really well if we could start by grabbing only the skin pixels from the image, and then working with those. Based on the features of a pixel (its blue, green, and red values) we want to know whether it is a skin pixel, or non-skin. Just like that, this has become an unsupervised classification problem. It's *classification* because there is a discrete set of values the outcome can take (two, in this case). It's *unsupervised* because we have no correctly-labelled training data to use. This type of problem is great to solve with k-means, which identifies points that have similar features, and groups them into k clusters. In this case, we are looking for two clusters: one is skin, and the other is everything else.\n",
    "\n",
    "Let's do some preliminary clustering. For each face, I've fit a k-means classifier, where k=2. Then I paint each pixel according to how it's been classified: any pixel assigned a 0 is colored red, and any pixel assigned 1 is colored blue. (The code for creating and saving these images is tedious at best, so I've left it out of this notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/red_and_blue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters map to the skin pretty well, but we can use a pretty simple trick to make the classification more accurate. Very few pixels on a person will be pure white, whereas the pixels that make up the background are all very close to white. Here, \"close\" refers to a pixel being numerically close to the value that represents white: (255, 255, 255). Before fitting the classifier, we can remove all of the white pixels. The subset of less extreme values will make the new means that are learned by the classifier more accurate.\n",
    "\n",
    "Below I've fit a k-means classifier (k=2) on the non-white pixels in an image. In the images, the white pixels that were left out when fitting the classifier are colored white, the pixels tagged \"0\" are colored red, and the pixels tagged \"1\" are colored blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/red_and_blue_white_background.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have three groups of pixels: background, skin, and other. Really, these are just two groups: skin, and non-skin. We need to combine the background pixels and other pixels into a single group. However, that is easier said than done. From the pictures above, you can see that sometimes the skin pixels are tagged \"0\" (colored red) and sometimes they are tagged \"1\" (colored blue). Right now, our classifier has no way of assigning the meaningful labels of skin and non-skin to these arbitrary tags.\n",
    "\n",
    "To programmatically decide which of the tags is associated with skin, we can make an assumption about the images: a square of pixels directly in the center of the image will probably be mainly skin. So we can grab a central patch of the photo, and classify all of it's pixels. The dominant label (either 0 or 1) is the \"skin label,\" and the picture can be colored according to that.\n",
    "\n",
    "Now we can squash the three original categories into just two categories, and look at the results. I've colored the skin pixels blue, and the non-skin pixels white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/blue_skin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the skin pixels have been identified, we can use those to calculate the average skin tone. Below are the averages calculated using just skin, compared to the original averages that were calculated using every pixel in the image. The new average is a much better approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/average_skin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the progression of an image from the face being detected, to the final average skin tone, with the original average for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/start_to_end.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that completes all of these steps for one face in order to calculate its average skin color. (I've omitted the lines that create colored images like above and save them into directories.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "def cluster_face(image, white_threshold):\n",
    "    \n",
    "    # save a patch of pixels from the center of the pic to identify the skin label\n",
    "    (height, width, three) = image.shape\n",
    "    (center_x, center_y) = (width // 2, height // 2)\n",
    "    patch = image[center_x - 10 : center_x + 10, center_y - 10 : center_y + 10]\n",
    "    flattened_patch = patch.transpose(2, 0, 1). reshape(3, -1).transpose()\n",
    "    \n",
    "    # remove white pixels, and fit a classifier\n",
    "    flattened = image.transpose(2, 0, 1).reshape(3, -1).transpose()\n",
    "    filtered = np.array([pixel for pixel in flattened if pixel.sum() < white_threshold])\n",
    "    k_means = KMeans(n_clusters=2)\n",
    "    k_means.fit(filtered)\n",
    "    \n",
    "    # identify the skin label\n",
    "    patch_labels = k_means.predict(flattened_patch)\n",
    "    skin_label = Counter(patch_labels).most_common()[0][0]\n",
    "    \n",
    "    # save the average skin pixel\n",
    "    all_labels = k_means.predict(filtered)\n",
    "    skin_mask = (all_labels == skin_label)\n",
    "    skin_pixels = filtered[skin_mask]\n",
    "    average_color = np.average(skin_pixels, axis=0)\n",
    "    return average_color\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/get_light.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's a single BGR value for each model representing her skin tone. But it's tricky to visualize triples. What's more, BGR values are for machines, and not for humans. When I look at how diverse a website seems, I'm not thinking about the levels of blue, green, and red in her skin tone, I'm thinking about how light or dark her skin is. So before trying to graph these machine-readable values, we need to translate them to numbers that make sense to humans.\n",
    "\n",
    "Inspired by [this article](https://pudding.cool/2018/06/makeup-shades/), each BGR triple can be converted to a [HLS triple](https://www.w3schools.com/colors/colors_hsl.asp). This means that instead of recording the blue, green, and red levels of a color, we record it's hue, lightness, and saturation. From there, we have a single numerical value (that is, a scalar rather than a vector) that represents how light the model's skin is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>company</th>\n",
       "      <th>b</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ae0000.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>106</td>\n",
       "      <td>130</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ae0001.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>113</td>\n",
       "      <td>133</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ae0002.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>126</td>\n",
       "      <td>158</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ae0003.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>151</td>\n",
       "      <td>172</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ae0004.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>152</td>\n",
       "      <td>170</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_name company    b    g    r\n",
       "0  ae0000.jpg      ae  106  130  188\n",
       "1  ae0001.jpg      ae  113  133  188\n",
       "2  ae0002.jpg      ae  126  158  217\n",
       "3  ae0003.jpg      ae  151  172  210\n",
       "4  ae0004.jpg      ae  152  170  211"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_values = pd.read_csv(\"bgr.csv\")\n",
    "color_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>company</th>\n",
       "      <th>b</th>\n",
       "      <th>g</th>\n",
       "      <th>r</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>ae0691.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>84</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.213725</td>\n",
       "      <td>0.541284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>ae0613.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>26</td>\n",
       "      <td>41</td>\n",
       "      <td>87</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>0.221569</td>\n",
       "      <td>0.539823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>gucci0086.jpg</td>\n",
       "      <td>gucci</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>87</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.249020</td>\n",
       "      <td>0.370079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>ae0693.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>44</td>\n",
       "      <td>57</td>\n",
       "      <td>95</td>\n",
       "      <td>0.042484</td>\n",
       "      <td>0.272549</td>\n",
       "      <td>0.366906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>ae0333.jpg</td>\n",
       "      <td>ae</td>\n",
       "      <td>43</td>\n",
       "      <td>59</td>\n",
       "      <td>102</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.284314</td>\n",
       "      <td>0.406897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_name company   b   g    r         h         l         s\n",
       "690      ae0691.jpg      ae  25  41   84  0.045198  0.213725  0.541284\n",
       "612      ae0613.jpg      ae  26  41   87  0.040984  0.221569  0.539823\n",
       "1120  gucci0086.jpg   gucci  40  55   87  0.053191  0.249020  0.370079\n",
       "692      ae0693.jpg      ae  44  57   95  0.042484  0.272549  0.366906\n",
       "333      ae0333.jpg      ae  43  59  102  0.045198  0.284314  0.406897"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_col = []\n",
    "l_col = []\n",
    "s_col = []\n",
    "\n",
    "for index, row in color_values.iterrows():\n",
    "    r = row[\"r\"] / 255\n",
    "    g = row[\"g\"] / 255\n",
    "    b = row[\"b\"] / 255\n",
    "    (h, l, s) = colorsys.rgb_to_hls(r, g, b)\n",
    "    h_col.append(h)\n",
    "    l_col.append(l)\n",
    "    s_col.append(s)\n",
    "    \n",
    "color_values[\"h\"] = h_col\n",
    "color_values[\"l\"] = l_col\n",
    "color_values[\"s\"] = s_col\n",
    "\n",
    "color_values = color_values.sort_values(by=[\"l\"])\n",
    "color_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready to plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/visualizing_diversity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*The code for making graphics is very precise and specific, and not very instructive, so I've omitted it to focus on the graphics themselves.*)\n",
    "\n",
    "First let's look at a palette of all the shades recorded, to see if there are any interesting patterns. The shades from American Eagle are pictured first, and Gucci next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/ae_all.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/gucci_all.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a pretty clear difference between the two companies: there are more dark shades from the Gucci website (especially percentage wise!) but you can see almost exactly where the dark shades drop off, and paler shades take over. In the American Eagle palette, there are plenty of \"medium\" colors. If we plot the shades, the drop-off in Gucci shades is very clear. American Eagle has a smoother curve.\n",
    "\n",
    "(The color for each bar is the average BGR value for all of the colors in that fall into that bucket of lightness values, so the color of any bar on the AE graph will be slightly different from the color of the corresponding bar on the Gucci graph.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graphs\"></a>\n",
    "![](display_pics/hists.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take a closer look at the shades in each of the buckets of lightness values by listing them all out, rather than averaging them together. Even though the format of the graphic is different, Gucci's bimodal distribution is still apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/new_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/takeaways.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was really surprised to see the overall shapes of the distributions. I did not expect a bimodal distribution at all, but after taking a look at the style prevalent on Gucci's website, it makes sense. The models they use have strikingly dramatic features, from their cheekbones to the color of their skin. This means selecting very dark and very light models. American Eagle's site, on the other hand, depicts models that fall into all sorts of ranges, which is why the distribution is smoother.\n",
    "\n",
    "This analysis also gave me a chance to look at how I assess a website's diversity. Just from scrolling, I decided that American Eagle's site contained a diverse range of models, and Gucci's was much less diverse. This is because while scrolling through the American Eagle site, I saw many more women of color, without thinking about the ratio of darker skinned models to total models. On Gucci's site, I saw fewer women of color, but it turns out that they actually make up a large percentage of the total images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/flaws.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a perfect analysis. However, each time I encountered a problem, I tried to address in the most appropriate way. Below are some of the things that I considered while working on this project, and some things that I would change or add if I had the chance to expand this project.\n",
    "\n",
    "\n",
    "**I didn't touch \"featured\" photos**\n",
    "\n",
    "The biggest thing that I would change about this project if I work on it in the future is to look at which models are \"featured\" at the top of each page. These models typically have a whole photo shoot, and their images are placed in prominent locations. By focusing only on the standard pictures of clothing, I brushed over another area where I could have looked at diversity.\n",
    "\n",
    "**Using multiple pictures of one model is different from using multiple pictures of multiple models**\n",
    "\n",
    "Many of the photos that I've used are repeats: they are different photos taken of the same model. At first, I considered only analyzing the distinct faces. Let's say a company has fifty white models and one black model. They post one picture each of the white models, and twenty of the black model. The code in this notebook would show that the website is pretty diverse, and it is, in one sense of the definition. But I wouldn't be comfortable calling a company that hires fifty times more white models than black models \"diverse\"-- it's a little more complicated than that. At the same time, the reverse could happen. A company could hire ten white models, and ten black models, and then post five hundred pictures featuring the white models, and ten pictures featuring the black models. In one type of analysis, this website would shine. In another type of analysis, they wouldn't. In the end, I decided that what is important to me is how often women of color are featured on the site, so that's what I focused on. But with more time, I would love to work with facial classification and identify the roster of models that a website has.\n",
    "\n",
    "**This analysis only includes two websites**\n",
    "\n",
    "Originally, I wanted to use close to a dozen different websites (it seems like there are infinite clothing websites nowadays), but since it was very difficult to generalize the web-scraping functions, the bulk of this project would have been spent wrangling whatever JavaScript the web designers implemented. I wanted to spend more time on the machine learning techniques and visualization.\n",
    "\n",
    "**The websites have a very different number of images**\n",
    "\n",
    "The nature of the companies (American Eagle sells a huge selection of casual clothes, while Gucci sells a curated collection of high fashion garments) means that the sites have different numbers of pictures. Some would say this is comparing apples to oranges. I'd argue that representation in fashion is important on all levels, and by using density rather than count in my histograms, I handle the difference well. What's more, the Gucci website still has a huge number of images: 300+ is a reasonable sample size for this type of analysis.\n",
    "\n",
    "**Only racial diversity is addressed**\n",
    "\n",
    "In fashion, ethnicity is only one area where diversity is lacking. Many companies favor thin models over plus-sized women, even when plus-sized models more accurately reflect the average customer. Whenever there are multiple areas of under-representation, there is also the issue of intersectionality. Perhaps a website has a reasonable amount of models of color and plus-sized models, but no plus-sized models of color. These are all important issues, but size is trickier to detect from images. I considered using edge detection to outline the women's bodies and compare this to the image size, and I considered scraping the model's names and sizes from different sources, but in the end decided that focusing on one area of underrepresentation was a more manageable task for a project this size.\n",
    "\n",
    "**Only a subset of racial diversity is addressed**\n",
    "\n",
    "While I look at the skin tones of the models, I don't look at their explicit race. A light-skinned Asian model may appear the same color as a white person; a mixed race model may appear the same color as an Indian person. There were two reasons that I was okay with looking at color rather than demographics. First, detecting race from a photo is messy and complicated. Second, skin color is an issue even within certain races. You can read about color-related beauty standards for [Asian](https://en.wikipedia.org/wiki/Light_skin_in_Japanese_culture), [Indian](https://thewire.in/culture/69275-tannishtha-chatterjee-comedy-night-bachao), and [Black](http://theconversation.com/black-americas-bleaching-syndrome-82200) women.\n",
    "\n",
    "**I only looked at the women's section of each site**\n",
    "\n",
    "Since the motivation for this project was anecdotal, and I don't ever look at the men's section, I didn't have the same motivation to analyze the diversity in men's photos. What's more, the men's sections of these sites had significantly fewer products, and I wanted to work with a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](display_pics/refs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started this project with a lot of theoretical knowledge, but very little practical knowledge. This meant a lot of googling. While I can't post every StackOverflow link that helped me solve an error, the following pages were instrumental in translating my high level understanding into workable code.\n",
    "\n",
    "* [Implementing facial recognition with `OpenCV`](https://realpython.com/face-recognition-with-python/)\n",
    "* [Scraping images from a website with `BeautifulSoup`](https://stackoverflow.com/questions/18304532/extracting-image-src-based-on-attribute-with-beautifulsoup)\n",
    "* [Using thresholds to pinpoint white pixels](https://www.mathworks.com/matlabcentral/answers/328835-how-can-i-extract-the-white-pixels-of-an-image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact me!\n",
    "\n",
    "Below are lots of ways to talk to me about this or other projects!\n",
    "\n",
    "* handamalaika (at) gmail.com\n",
    "* [Twitter](twitter.com/malicodes)\n",
    "* [LinkedIn](https://www.linkedin.com/in/malaika-handa/)\n",
    "* [Resume](https://drive.google.com/file/d/1ammnktVHH-GdZ2UOsGCsUS_VM3w8csh8/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
